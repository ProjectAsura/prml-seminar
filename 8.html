<!doctype html>
<html lang="ja">

	<head>
		<meta charset="utf-8">

    <title>パターン認識・機械学習勉強会 第8回 @ ワークスアプリケーションズ</title>

		<meta name="description" content="Seminar of category theory">
    <meta name="author" content="Koichi Nakamura">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/beige.css" id="theme">

    <meta http-equiv="X-UA-Compatible" CONTENT="IE=EmulateIE7" />

		<!-- For syntax highlighting -->
    <link rel="stylesheet" href="plugin/highlight/styles/github.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
      });
    </script>

    <style type="text/css">
      <!--
      div.definition {
        padding-left: 10px;
        padding-right: 10px;
        border: 4px solid #333333;
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.15);
      }

      .reveal .chapter-title {
        margin-top: 3em;
      }

      .reveal {
        font-size: 36px;
        line-height: 1.4em;
      }

      .reveal .slides {
        text-align: left;
      }

      .reveal section img {
        border: none;
        background: 0;
        margin-left: 1em;
        margin-right: 1em;
        box-shadow: none;
      }

      .reveal strong {
        color: #ff6666;
      }

      .reveal sup {
        font-size: 40%;
      }

      .reveal .note {
        font-size: 40%;
      }

      .reveal .controls div.navigate-up,
      .reveal .controls div.navigate-down {
        display: none;
      }

      .reveal .block {
        border: solid 2px;
        position: relative;
        border-radius: 8px;
        margin: 0.5em;
        padding: 1em 0.8em 0.5em 0.8em;
      }

      .reveal .block:after {
        content: "";
        display: block;
        clear: both;
        height: 1px;
        overflow: hidden;
      }
      --> 
    </style>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

        <section>
        <h2>パターン認識・<br> 機械学習勉強会 <br> 第8回</h2>
        <h3>@ワークスアプリケーションズ</h3>
        <small> 中村晃一 <br> 2014年4月10日 </small>
        </section>

        <section>
        <h3>謝辞</h3>
        <p>
        この会の企画・会場設備の提供をして頂きました<br>
        &#12849; ワークスアプリケーションズ様<br>
        にこの場をお借りして御礼申し上げます.
        </p>
        </section>

        <section>
        <h2 class="chapter-title"> ニューラルネットワーク </h2>
        </section>

        <section>
        <p>
        前回に続きニューラルネットワークの解説を行います. 5.3節以降技術的な話題が多くなりますが、特に重要なものに絞ります.
        </p>
        </section>

        <section>
        <h3> 実装の検証 </h3>
        <p>
        誤差逆伝播法はニューラルネットワークに対する何らかの誤差関数 $E(\mathbf{x},\mathbf{w})$ の, 重み $w_{ji}$ に関する微分係数
        \[ \frac{\partial E}{\partial w_{ji}} \]
        を計算するものでした.
        </p>
        <p class="fragment">
        一方, <strong> 数値微分法 (numerical differentiation) </strong> によってこれを近似的に計算する事も出来ます. 2つの結果を比較する事によって実装の検証を行います.
        </p>
        </section>

        <section>
        <p>
        例えば, <strong> 中心差分法 (central difference) </strong> では, 微小量 $\varepsilon &gt; 0$ に対して
        \[ \frac{\partial E}{\partial w_{ji}} = \frac{E(w_{ji} + \varepsilon)-E(w_{ji} - \varepsilon)}{\varepsilon^2} + O(\varepsilon^2) \]
        となります(<a href="http://nineties.github.io/math-seminar/3.html#/39">参考</a>).
        </p>
        <p class="fragment">
        各微分係数の計算に2回の順伝播が必要なので, 計算量は重みの数 $W$ に対して $\mathcal{O}(W^2)$ となります.
        </p>
        </section>

        <section>
        <p>
        前回実装したものを中心差分の結果と比較した所以下のようになりました。実装は誤っていないようです。 (<a href="prog/prog8-1.py" style="font-size:80%">prog8-1.py</a>)
        </p>
<pre><code class="python" style="max-height:400px">誤差逆伝播で求めた微分係数
[  3.40753294  27.62924471   4.04890811 -14.67453553  -5.83120129
  32.10422817  40.14607951  40.10717238  24.84976142  58.85584348]
中心差分 (eps=0.010000) で求めた微分係数
[  3.40736966  27.62873587   4.04890491 -14.67486427  -5.83108375
  32.10315101  40.14607951  40.10717238  24.84976142  58.85584348]
</code></pre>
        </section>

        <section>
        <h3> ヤコビ行列 </h3>
        <p>
        ニューラルネットワーク $\mathbf{y}=f(\mathbf{x},\mathbf{w})$ に対して,
        \[ J_{ij} = \frac{\partial y_i}{\partial x_j} \]
        という微分係数, は入力の変動に対する, 出力の変動を表します.
        </p>
        <p>
        $J_{ij}$ を $(i,j)$ 成分にもつ行列は <strong> ヤコビ行列 (Jacobian matrix) </strong> と呼ばれます.
        </p>
        </section>

        <section>
        <p>
        ヤコビ行列が必要となる典型的な場面は <strong> 不変性 (invariance) </strong> の評価です.
        </p>
        <p class="fragment" data-fragment-index="1">
        例えば文字認識の問題を考えましょう. 文字画像(入力)に平行移動・回転・拡大などの変換を行ってもそのクラス(出力)は不変です.
        </p>
        <div class="fragment" data-fragment-index="1" align="center"> <img width="900px" src="fig/invariance.png"> </div>
        </section>

        <section>
        <p>
        出力 $y_k$ は, 入力 $x_i$ に $x_i$ と直接接続する素子の入力 $a_j$ を介してのみ依存するので
        \[ J_{ki} = \frac{\partial y_k}{\partial x_i} = \sum_j \frac{\partial a_j}{\partial x_i}\frac{\partial y_k}{\partial a_j} = \sum_j w_{ji}\frac{\partial y_k}{\partial a_j} \]
        となります.
        </p>
        <div align="center"> <img width="400px" src="fig/neural-network8.png"> </div>
        </section>

        <section>
        <p>
        誤差逆伝播法によって $\delta_{ji} = \partial y_j/\partial a_i$ を計算することは出来るので, 以下の手順でヤコビ行列を計算出来ます.
        </p>
        <div class="block" style="border-color:blue;font-size:90%">
        <h4> ヤコビ行列の計算 </h4>
        <p>
        誤差逆伝播法によって
        \[ \delta_{ji} = \frac{\partial y_j}{\partial a_i} \]
        を求める. 続いて各入力素子 $i$ と出力素子 $k$ に対して
        \[ J_{ki} = \sum_j w_{ji}\delta_{kj} \]
        を求める. 但し 素子 $j$ は入力 $i$ に接続する素子の番号.
        </p>
        </div>
        </section>

        <section>
        <p>
        $y=x^2$ を学習させた例で試してみましょう.
        \[ \frac{\mathrm{d} y}{\mathrm{d} x} = 2x \]
        となるはずです.
        </p>
        <div align="center"> <img width="500px" src="prog/fig7-1-quadratic.png"> <a href="prog/prog7-1.py" style="font-size:60%">prog7-1.py</a> </div>
        </section>

        <section>
        <p>
        以下のようなネットワークでしたので, 出力層については
        \[ \frac{\partial y}{\partial a} = 1 \]
        で計算します.
        </p>
        <div align="center"> <img width="400px" src="fig/neural-network6.png"> </div>
        </section>

        <section>
        <p>
        以下が計算結果のプロットです. 確かに $y=2x$ になっている事が分かります. 定義域の境界では関数が不連続であるので誤差が大きくなります.
        </p>
        <div align="center"> <img width="600px" src="prog/fig8-2.png"> <a href="prog/prog8-2.py" style="font-size:60%">prog8-2.py</a> </div>
        </section>

        <section>
        <h3> 準ニュートン法 </h3>
        <p>
        前回は最急降下法を実装しましたが. これは1ステップの更新に掛かる時間が $\mathcal{O}(W)$ で済む一方, 収束するまでの反復回数が多くなってしまいます.
        </p>
        <p class="fragment">
        より効率的な手法として <strong> 共役勾配法 (conjugate gradient method) </strong> や <strong> 準ニュートン法 (quasi-Newton method) </strong> などがあります.
        </p>
        <p class="fragment">
        第５回にニュートン・ラフソン法を解説しましたので, 今回はそれに基づく準ニュートン法を説明します. 共役勾配法については各自調べて下さい.
        </p>
        </section>

        <section>
        <p>
        ニュートン・ラフソン法を復習すると, 適当な初期値 $\mathbf{w}_0$ を定め,
        \[ \mathbf{w}^{(k+1)} = \mathbf{w}^{(k)} - \left(\mathbf{H}^{(k)}\right)^{-1}\nabla \mathbf{E}^{(k)} \]
        によって反復を行う方法でした. $\nabla \mathbf{E}^{(k)}$ はステップ $k$ での誤差関数の勾配, $\mathbf{H}^{(k)}$ はステップ $k$ での誤差関数のヘッセ行列です.
        </p>
        <p class="fragment">
        パラメータ数が $W$ の時, $\mathbf{H}^{(k)}$ は $W\times W$ 行列になるので計算量が問題となります. そこで $\left(\mathbf{H}^{(k)}\right)^{-1}$ を別の近似行列に置き換えたものが 準ニュートン法です.
        </p>
        </section>

        <section>
        <h3> ヘッセ行列の近似 </h3>
        <p>
        テキスト 5.4 節にはヘッセ行列 $\left(\mathbf{H}^{(k)}\right)^{-1}$ の各種近似計算法が紹介されていますが, ここでは <strong> 外積による近似 (outer product approximation) </strong> を紹介します.
        </p>
        </section>

        <section>
        <p>
        出力が1つの場合を考えると, 誤差関数が残差平方和の半分
        \[ E = \frac{1}{2}\sum_{n=1}^N (y_n-t_n)^2 \]
        の場合
        \[ \nabla E = \sum_{n=1}^N (y_n-t_n)\nabla y_n \]
        なので
        \[ \mathbf{H} = \nabla\nabla E = \sum_{n=1}^N \nabla y_n (\nabla y_n)^T + \sum_{n=1}^N (y_n-t_n)\nabla\nabla y_n \]
        となります.
        </p>
        </section>

        <section>
        <p>
        ここで, $E$ の最小値の付近では $\nabla y_n \approx \mathbf{0}$ になっているので $\nabla \nabla y_n \approx \mathbf{O}$ ですから
        \[ \mathbf{H} \approx \sum_{n=1}^N\nabla y_n (\nabla y_n)^T \]
        と近似出来ます. これを外積による近似と言います.
        </p>
        </section>

        <section>
        <p>
        出力が $K &gt; 1$ 個の場合も同様にして
        \[ \mathbf{H} \approx \sum_{n=1}^N\sum_{i=1}^K\nabla y_{ni} (\nabla y_{ni})^T \]
        と近似出来ます.
        </p>
        </section>

        <section>
        <p>
        今,
        \[ \mathbf{H}_N = \sum_{n=1}^N\mathbf{b}_n\mathbf{b}_n^T \qquad (\mathbf{b}_n = \nabla y_n)\]
        とおきます.
        </p>
        <p>
        これをデータ1つずつについて漸進的に計算出来るように
        \[ \mathbf{H}_{L+1} = \mathbf{H}_L + \mathbf{b}_{L+1}\mathbf{b}_{L+1}^T \]
        という形に直します.
        </p>
        </section>

        <section>
        <p>
        ここで
        \[ (\mathbf{H}+\mathbf{b}\mathbf{b}^T)^{-1} = \mathbf{H}^{-1} - \frac{\mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T\mathbf{H}^{-1}}{1+\mathbf{b}^T\mathbf{H}^{-1}\mathbf{b}} \]
        が成立するので(次頁),
        \[ \mathbf{H}_{L+1}^{-1} = \mathbf{H}_L^{-1}-\frac{\mathbf{H}_L^{-1}\mathbf{b}_{L+1}\mathbf{b}_{L+1}^T\mathbf{H}_L^{-1}}{1+\mathbf{b}^T_{L+1}\mathbf{H}_L^{-1}\mathbf{b}_{L+1}} \]
        となります.
        </p>
        <P>
        但し, $\mathbf{H}_0$ は小さい $\alpha$ に対して $\mathbf{H}_0 \approx \alpha I$ と近似します.
        </p>
        </section>

        <section style="font-size:80%">
        <p>
        【前頁の等式の証明】<br>
        まず
        \[ \mathbf{H}^{-1}(\mathbf{H}+\mathbf{b}\mathbf{b}^T) = I + \mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T \qquad\cdots(1)\]
        また,
        \[ \begin{aligned}
        \mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T\mathbf{H}^{-1}(\mathbf{H}+\mathbf{b}\mathbf{b}^T) &=
        \mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T+\mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T\mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T\\
        &= \mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T+(\mathbf{b}^T\mathbf{H}^{-1}\mathbf{b})\mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T\\
        &  \qquad\qquad \text{($\because\, \mathbf{b}^T\mathbf{H}^{-1}\mathbf{b}$はスカラー)} \\
        &= (1+\mathbf{b}^T\mathbf{H}^{-1}\mathbf{b})\mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T
        \end{aligned} \]
        より
        \[ \frac{\mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T\mathbf{H}^{-1}}{1+\mathbf{b}^T\mathbf{H}^{-1}\mathbf{b}}(\mathbf{H}+\mathbf{b}\mathbf{b}^T) = \mathbf{H}^{-1}\mathbf{b}\mathbf{b}^T \qquad\cdots(2)\]
        $(1)$から$(2)$を引いて終了.
        </p>
        </section>

        <section>
        <h3> 準ニュートン法の計算量 </h3>
        <p>
        $\mathbf{A},\mathbf{B}$ が $n\times n$ 行列, $\mathbf{x},\mathbf{y}$ が $n$次元ベクトルの時のこれら積の計算量は以下のようになります.
        </p>
        <ul>
          <li> $\mathbf{A}\mathbf{B}$ は $\mathcal{O}(n^3)$ 時間 </li>
          <li> $\mathbf{A}\mathbf{x}, \mathbf{x}^T\mathbf{A}$ は $\mathcal{O}(n^2)$ 時間 </li>
          <li> $\mathbf{x}\mathbf{x}^T$ は $\mathbf{O}(n^2)$ 時間 </li>
        </ul>
        <p>
        従って, 準ニュートン法の1ステップの更新は
        \[ \mathbf{H}_{L+1}^{-1} = \mathbf{H}_L^{-1}-\frac{((\mathbf{H}_L^{-1}\mathbf{b}_{L+1})\mathbf{b}_{L+1}^T)\mathbf{H}_L^{-1}}{1+(\mathbf{b}^T_{L+1}\mathbf{H}_L^{-1})\mathbf{b}_{L+1}} \]
        という順序で積を計算すると $\mathcal{O}(W^2)$ となります.
        </p>
        </section>

        <section>
        <h3> 注意 </h3>
        <p>
        \[ \mathbf{H} \approx \sum_{n=1}^N\nabla y_n (\nabla y_n)^T \]
        という近似が出来るのは誤差関数 $E$ の値がある程度極大値・極小値付近に近づいた場合です. そこで, 準ニュートン法の前に再急降下法で適当回数慣らし運転を行う事にしました.
        </p>
        </section>

        <section>
        <p> 前回の問題に適用してみます. </p>
        <div align="center"> <img width="700px" src="prog/fig8-4-quadratic.png"> <a href="prog/prog8-4.py" style="font-size:60%">prog8-4.py</a> </div>
        </section>
        <section>
        <div align="center"> <img width="700px" src="prog/fig8-4-sin.png"> <a href="prog/prog8-4.py" style="font-size:60%">prog8-4.py</a> </div>
        </section>
        <section>
        <div align="center"> <img width="700px" src="prog/fig8-4-abs.png"> <a href="prog/prog8-4.py" style="font-size:60%">prog8-4.py</a> </div>
        </section>
        <section>
        <div align="center"> <img width="700px" src="prog/fig8-4-heaviside.png"> <a href="prog/prog8-4.py" style="font-size:60%">prog8-4.py</a> </div>
        </section>

        <section>
        <p>
        今紹介した手法では, 局所解に陥ってしまう可能性がある事に注意する必要があります. 現在の点に関する局所的な情報しか見ていないからです. それ以前に収束するか否かも保証されません.
        </p>
        <div align="center"> <img width="500px" src="fig/local_optimal.png"> </div>
        </section>

        <section>
        <p style="font-size:80%">
        以下のグラフは先ほどの手法で $y=\sin \pi x$ のフィッティングを10回行ったものです. $\mathbf{w}$ の初期値をランダムに決めているので毎回結果が異なりますが, 2種類の曲線(実際には4種類が重なっている)に収束しています. これらが局所解の例.
        </p>
        <div align="center"> <img width="700px" src="prog/fig8-5.png"> <a href="prog/prog8-5.py" style="font-size:60%">prog8-5.py</a> </div>
        </section>

        <section>
        <h3> 直線探索の利用 </h3>
        <p>
        局所解を避ける為には大域的な情報を利用する必要があります.
        </p>
        <p>
        しかし全空間を調べあげる事はとても出来ません. そこで <strong>線形探索 (line search)</strong>という手法が使えます.
        これは,方向を決めた後進む距離だけを大域的な探索によって決定するというものです.
        </p>
        <div align="center"> <img width="500px" src="fig/local_optimal2.png"> </div>
        </section>

        <section>
        <div class="block" style="border-color:blue;font-size:90%">
        <h4> 直線探索の併用 </h4>
        <ol>
          <li> 次に進む方向 $\mathbf{p}^{(k)}$ を何らかの方法(準ニュートン法など)で決める. ($\mathbf{p}^{(k)}$ は $E$ の降下方向を向いていなければいけない.) </li>
          <li> $E(\mathbf{w}^{(k)}+\alpha\mathbf{p}^{(k)})$ が最小となるステップ幅 $\alpha$ を何らかの直線探索で求める. </li>
          <li> $\mathbf{w}^{(k+1)} = \mathbf{w}^{(k)}+\alpha\mathbf{p}^{(k)}$ とする. </li>
        </ol>
        </div>
        </section>

        <section>
        <p>
        ここで, $E(\mathbf{w}+\alpha\mathbf{p})$ が最小となるステップ幅 $\alpha$ を厳密に求める必要はありません. <strong> Armijoの条件 (Armijoの条件)</strong> と <strong> Wolfeの条件 (Wolfe condition) </strong> と呼ばれるものを満たせば大域的な収束性が証明されます.
        </p>
        <div align="center"> <img width="600px" src="fig/local_optimal3.png"> </div>
        </section>

        <section>
        <p>
        $f(\alpha) = E(\mathbf{w}+\alpha\mathbf{p})$ とおきます.
        </p>
        <p>
        まず, $0 &lt; c_1 &lt; 1$ を満たす小さな $c_1$に対して
        \[ \frac{f(\alpha)-f(0)}{\alpha} \leq c_1 f'(0) \]
        が成立する必要があります. これは $\alpha$ の上限を定め, Armijoの条件と呼ばれます.
        </p>
        <div align="center"> <img width="600px" src="fig/armijo-rule.png"> </div>
        </section>

        <section>
        $f(\alpha)=E(\mathbf{w}+\alpha\mathbf{p})$ を代入すると
        \[ f'(0) = \lim_{\alpha\rightarrow 0}\frac{E(\mathbf{w}+\alpha\mathbf{p})-E(\mathbf{w})}{\alpha} = \mathbf{p}^T\nabla E(\mathbf{w}) \]
        となるので(<a href="http://nineties.github.com/math-seminar/3.html#/58">参考</a>), Armijoの条件を整理すると
        \[ E(\mathbf{w}+\alpha\mathbf{p})\leq E(\mathbf{w})+c_1\alpha\mathbf{p}^T\nabla E(\mathbf{w}) \]
        となります.
        </section>

        <section>
        <p>
        続いて $0 &lt; c_1 &lt; c_2 &lt; 1$ を満たす比較的大きい $c_2$ に対して
        \[ f'(\alpha) \geq c_2f'(0) \]
        を満たす必要があります. これは $\alpha$ の下限を定め, Wolfeの条件と呼ばれます.  こちらも整理すると
        \[ \mathbf{p}^T\nabla E(\mathbf{w}+\alpha\mathbf{p}) \geq c_2\mathbf{p}^T\nabla E(\mathbf{w}) \]
        となります.
        </p>
        <div align="center"> <img width="600px" src="fig/wolfe-rule.png"> </div>
        </section>

        <section>
        <p>
        まとめると以下の様になります.
        </p>
        <div class="block" style="border-color:blue;font-size:90%">
        <h4> Armijo,Wolfeの条件によるステップサイズの決定</h4>
        <ol>
          <li> $\alpha &gt; 0$ を適当に決める. </li>
          <li> Armijoの条件を満たさないなら $\alpha$ は大きすぎるのでより小さくし最初に戻る.
          \[ E(\mathbf{w}+\alpha\mathbf{p})\leq E(\mathbf{w})+c_1\alpha\mathbf{p}^T\nabla E(\mathbf{w}) \]
          </li>
          <li> Wolfeの条件を満たさないなら $\alpha$ は小さすぎるのでより大きくし最初に戻る.
          \[ \mathbf{p}^T\nabla E(\mathbf{w}+\alpha\mathbf{p}) \geq c_2\mathbf{p}^T\nabla E(\mathbf{w}) \]
          </li>
        </ol>
        </div>
        </section>

        <section>
        <p style="font-size:80%">
        $y=\sin \pi x$ の例に適用して見ました. 準ニュートン法と同じく $\mathbf{p}=-\mathbf{H}^{-1}\nabla E$ としました.
        以下のグラフでは50回のフィッティングを行っていますが局所解を回避する事が出来ました.
        </p>
        <div align="center"> <img width="700px" src="prog/fig8-6.png"> <a href="prog/prog8-6.py" style="font-size:60%">prog8-6.py</a> </div>
        </section>

        <section>
        <h3> 不変性 </h3>
        <p>
        続いて, 先ほどヤコビ行列の所で紹介した <strong>不変性 (invariance)</strong> について詳しく説明します.
        </p>
        <div align="center"> <img width="900px" src="fig/invariance.png"> </div>
        </section>

        <section>
        <p>
        不変性をネットワークの学習に取り入れるには以下の方法があります.
        </p>
        <ul>
          <li> 対応する変換を用いて訓練パターンを複数生成して学習を行わせる. 例えば画像が回転に関して不変なら, 元画像を回転させた訓練データを複数用いる. </li>
          <li> 対応する変換に関して出力が変化してしまう事に対してペナルティを付与し学習を行わせる. </li>
          <li> 前処理の特徴抽出の段階でこのような不変性は除去しておく. </li>
          <li> ネットワークの構造自体に不変性を組み込む. </li>
        </ul>
        </section>

        <section>
        <h3> 接線伝播法 </h3>
        <p>
        「出力が変化してしまう事に対するペナルティ」の考え方に基づく <strong> 接線伝播法 (tangent propagation) </strong> を紹介します.
        </p>
        </section>

        <section>
        <p>
        今, 入力画像 $\mathbf{x}_n$ に変換 $\mathbf{s}(\mathbf{x},\boldsymbol{\theta})$ を施しても出力が不変だとしましょう. ここで $\boldsymbol{\theta}$ は変換のパラメータであり, 例えば回転角や平行移動の差分ベクトルなどです. 但し, $\mathbf{s}(\mathbf{x},\mathbf{0})=\mathbf{x}$ とします.
        </p>
        <div align="center"> <img width="500px" src="fig/tangent_propagation.png"> </div>
        </section>

        <section>
        <p>
        この時, ニューラルネットワーク
        \[ \mathbf{y} = f(\mathbf{x},\mathbf{w}) \]
        の学習データ $\mathbf{x}_n$ と変換 $\mathbf{s}$ に対する不変性は, $\boldsymbol{\theta}$ を変えても出力が変化しない事. すなわち,
        \[ \nabla \mathbf{y} = \frac{\partial\mathbf{y}}{\partial\boldsymbol{\theta}} = \frac{\partial}{\partial\boldsymbol{\theta}}f\left(\mathbf{s}(\mathbf{x}_n,\boldsymbol{\theta}),\mathbf{w}\right) = \mathbf{O} \]
        と表現する事が出来ます.
        </p>
        </section>

        <section>
        <p>
        合成微分則を用いて書き直せば, 出力 $y_j$ と $\boldsymbol{\theta}$ の成分 $\theta_i$ について
        \[ \frac{\partial y_j}{\partial \theta_i} = \sum_{k=1}^D\frac{\partial y_j}{\partial x_k}\frac{\partial x_k}{\partial \theta_i} \]
        ヤコビ行列を用いて
        \[ \frac{\partial y_j}{\partial \theta_i} = \sum_{k=1}^D\mathbf{J}_{jk}\frac{\partial x_k}{\partial \theta_i} \]
        と書くことが出来ます.
        </p>
        </section>

        <section>
        <p>
        更に $x_k = s_k(\mathbf{x}_n,\boldsymbol{\theta})$ だったので
        \[ \frac{\partial y_j}{\partial \theta_i} = \sum_{k=1}^D\mathbf{J}_{jk}\frac{\partial s_k}{\partial \theta_i} \]
        という形になります.
        </p>
        <p>
        もっと簡潔に書けば
        \[ \frac{\partial\mathbf{y}}{\partial\boldsymbol{\theta}}=\mathbf{J}\frac{\partial\mathbf{s}}{\partial\boldsymbol{\theta}} \]
        という形になります.
        </p>
        </section>

        <section>
        <p>
        ここで $\mathbf{x}_n$ を固定して, $\boldsymbol{\theta}$ を動かすと
        \[ \mathbf{x} = s(\mathbf{x}_n, \boldsymbol{\theta}) \]
        は入力空間内の何らかの多様体(曲線,曲面,$\cdots$)を描きますが, $\partial\mathbf{s}/\partial\boldsymbol{\theta}$ はこの多様体に接するベクトルとなります. これが「接線伝播」という名前の由来です.
        </p>
        <div align="center"> <img width="500px" src="fig/tangent_propagation2.png"> </div>
        </section>

        <section>
        <p>
        さて, $\boldsymbol{\theta}$ 全体で
        \[ \frac{\partial\mathbf{y}}{\partial\boldsymbol{\theta}}=\mathbf{O} \]
        を満たすようにするのは困難なので, $\boldsymbol{\theta}=\mathbf{0}$ の周辺(つまり入力 $\mathbf{x}_n$ の周辺)だけ考え,
        \[ E(\mathbf{w}) + \lambda \left\Vert\mathbf{J}\frac{\partial\mathbf{s}}{\partial\boldsymbol{\theta}}\right\Vert^2 \]
        という量を最小化する事にします. $E(\mathbf{w})$ は誤差関数, $\lambda$ は適当な正の値, $\Vert \cdot \Vert^2$ は各成分の二乗和(フロベニウスノルム)です.
        </p>
        </section>

        <section>
        <p>
        資料作成途中.
        </p>
        </section>

        <!--
        <section>
        <h3> 正則化 </h3>
        <p>
        さて, ニューラルネットワークによって表されるモデルの複雑さは隠れ層の素子の数 $M$ によって決まります. 既に説明したように, $M$ の大小によって汎化性能が変わります.
        </p>
        </section>

        <section>
        <p>
        以下のデータで見てみましょう.
        </p>
        <div align="center"> <img width="700px" src="prog/fig8-3-training.png"> <a href="prog/prog8-3.py" style="font-size:60%">prog8-3.py</a> </div>
        </section>

        <section>
        <p> $M = 1$ </p>
        <div align="center"> <img width="700px" src="prog/fig8-3-1.png"> <a href="prog/prog8-3.py" style="font-size:60%">prog8-3.py</a> </div>
        </section>
        <section>
        <p> $M = 2$ </p>
        <div align="center"> <img width="700px" src="prog/fig8-3-2.png"> <a href="prog/prog8-3.py" style="font-size:60%">prog8-3.py</a> </div>
        </section>
        <section>
        <p> $M = 3$, 過学習が始まっています. </p>
        <div align="center"> <img width="700px" src="prog/fig8-3-3.png"> <a href="prog/prog8-3.py" style="font-size:60%">prog8-3.py</a> </div>
        </section>

        <section>
        <p>
        この対策としては既に説明したような
        </p>
        </section>
        -->
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: false,
				progress: true,
				history: true,
				center: true,
        rollingLinks: false,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/fade/none

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/showdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});
      Reveal.addEventListener( 'slidechanged', function( event ) {
        MathJax.Hub.Rerender(event.currentSlide);
      });

		</script>

	</body>
</html>
